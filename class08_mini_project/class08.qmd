---
title: "Class 8: Machine Learning Mini Project"
author: "Cynthia Lin"
format: gfm
---

In today's mini project we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering and PCA for now). 

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data.

##Data Import


```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

# Save your input data file into your Project directory
```{r}
fna.data <- "WisconsinCancer.csv"
head(wisc.df)
```
Remove the diagnosis column and keep it in a separate vector for later.
```{r}
diagnosis <-  as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

##Exploratory data analysis
The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data. 

> Q1. How many obsrevations are in this dataset?

```{r}
nrow(wisc.data)
```


> Q2. How many of the obserations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many vairables/features in the data are suffixed with _mean?

first find the column names
```{r}
colnames(wisc.data)
```

Next I need to search within the column names for "_mean" pattern. The `grep()` function might help here.

```{r}
inds <- grep("_mean", colnames(wisc.data))
length(inds)
```
> Q. How many dimesnions are in this dataset?

```{r}
ncol(wisc.data)
```
# Principal Component Analysis

First do we need to scale the data before PCA or not.


```{r}
apply(wisc.data, 2, sd)
```
LOOKS LIKE WE NEED TO SCALE.

```{r}
#Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```


>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs capture 72%

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 principal components are required to describe at least 90% of the original variance in the data.

## PC plot

we need to make our plot PC1 vs PC2 (aka score plo, PC-plot, etc.). The main result of PCA...

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```



```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The biplot is difficult to understand as everything is crowded together and overlapping each other. It is impossible to read and decipher what it says or interpret much from this plot. 

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

This graph is good because we can see a separation of the malignant and bengign samples. We can also see the clustering. 

# using ggplot 

```{r}
library(ggplot2)
pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <- diagnosis
ggplot(pc, aes(PC1, PC2, col=diagnosis))  + geom_point()
```

This PCA plot shows a separation of the Malignant from Benign samples. Malignant is blue and red is Benign. Each point is a sample and those with similar characteristics should cluster together. 

## variance explained

We can get this form the output of the `summary()` function. 

```{r}
summary(wisc.pr)
```

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (ie wisc.pr$sdev^2) save the result as an object called pr.var.

```{r}
#calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. 

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 0.5), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

#Examine the PC loadings

How much do the original variables contribute to the new PCs that we have calculated? To get at this data we can look at the `$rotation` component of the returned PCA object. 

```{r}
head(wisc.pr$rotation[,1:3])
```

Focus on PC1
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

The component of the loading vector for the feature concav.points_mean is 

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

There is a complicated mix of variables that go together to make up PC1 = i.e. there are many of the original variables that together contribute highly to PC1. 

```{r}
loadings <- as.data.frame(wisc.pr$rotation)
ggplot(loadings) + aes(PC1, rownames(loadings)) + geom_col()
```


>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

The minimum number of principal components required to explain 80% of the variance of the data is 3 principal components.

## Hierarchal Clustering

The goal of this section is to do hierarchal clusterng or the original data. 

First we will scale the data, then distance matrix, then hclust
```{r}
wisc.hclust <- hclust( dist( scale (wisc.data)))
```

```{r}
plot(wisc.hclust)
```
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust) + abline(h=19.5, col="red", lty=2)
```

>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?


Cut this tree to yield cluster membership vector with `cutree()` function. 

```{r}
grps <- cutree(wisc.hclust, h=19)
table(grps)
```

```{r}
table(grps, diagnosis)
```
>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I like the method where we used `ward.D2` gives my favorite results for the same data.dist dataset because it minimizes the amount of variance within the clusters. It also looks more separated and organized in my opinion. 


#Combine methods: PCA and HCLUST

My PCA results were interesting as they showed a separation of M and B samples along PC1. 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()` '

Try clustering in 3 PCs, that is PC1, PC2, PC3 as input
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
```
My resulting figure
```{r}
plot(wisc.pr.hclust)
```
Lets cut this tree into 2 groups/clsuters

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

How well do the two clusters separate the M and B diagsosis
```{r}
table(grps, diagnosis)
```

```{r}
(179 + 333) / nrow(wisc.data)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
d <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(d, method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)

```
I think that the newly created model with four clusters separates out the two diagnoses well because it is more concise and clear. I am more able to see the clustering and separation of the malignant and benign groups compared to earlier models. 






